---
title: "Cloud computing overview"
author: "emLab"
output:
  revealjs::revealjs_presentation:
    theme: simple
    center: true
    transition: none
    background_transition: none
    reveal_options:
      slideNumber: 'c/t'
date: "`r Sys.Date()`"
---

## Why cloud computing?

- Running analyses in parallel 
  + (where more cores are good!)   

- Running analyses with data that are too big for your personal computer 
  + (where more RAM is good!)  
  
- Freeing up your personal computer to do other things  

## Google virtual machines (VM) using Compute Engine  

- Uses Google's [Compute Engine](https://cloud.google.com/compute), part of the Google Cloud ecosystem of products  

- Spin up virtual machines (VMs) while specifying the number of cores you want and how much RAM you want  

  + Up to 224 cores and 896 GB RAM!

- They cost money to use, but [pricing](https://cloud.google.com/compute/vm-instance-pricing) is straightforward and on a per-hour basis  

  + 16 cores and 64 GB of RAM costs \$0.54/hour
  + 32 cores and 128 GB of RAM costs \$1.08/hour
  + 224 cores and 896 GB RAM costs \$7.57/hour

## Google virtual machines (VM) using Compute Engine  

- Three ways to spin up VMs: 
  + [Google's online GUI](https://console.cloud.google.com/compute)  
  + [The terminal](https://grantmcdermott.com/ds4e/gce-i.html)  
  + [googleComputeEngineR](https://cloudyr.github.io/googleComputeEngineR/) R package
  
- Can install R Studio, allowing you to run R Studio interactively through your web browser

## UCSB high performance computing clusters

- Uses the Pod, a high performance computing cluster, available through the [UCSB Center for Scientific Computing](https://csc.cnsi.ucsb.edu/) (CSC)  

- The pod has 71 nodes:  

  + 64 regular nodes each with 40 cores  
  + 4 large memory nodes with more than 1TB of RAM  
  + 3 Graphic Processing Unit (GPU) nodes with 4 32GB V100s connected via NVLink (for image and video processing and some types of machine learning)  

- It's free! 

  + The clusters are funded through an NSF grant and free to use for USCB faculty with proper [citation](https://csc.cnsi.ucsb.edu/publications) in any publications resulting from their use 

## UCSB high performance computing clusters

 - Getting started on pod:  
 
  + Register for an [account](https://csc.cnsi.ucsb.edu/forms/user-account)     
  + Access via the terminal    
  + With [X2GO](https://csc.cnsi.ucsb.edu/docs/using-x2go-gui-login-knot-or-pod) client   
  + With [Visual Studio Code](https://code.visualstudio.com/) remote connections    
  + Off-campus: can use any of the above methods but have to connect to the campus VPN first  
  
## Working with data on Google Drive

- Manually move data   
  + Applications like [FileZilla](https://filezilla-project.org/) can help with transferring data between local and remote servers 
- [googledrive](https://googledrive.tidyverse.org/index.html) R package to read/write data from the emLab Team Drive
- Mount Google Drive as disk (*not currently working on Google Compute Engine*)

## Comparing the two approaches

- Cost
- Computing power
- Handling data
- Working with data on Google Drive
- Ease of use

## Our recommendation (*for most applications*)

Google VMs using Compute Engine

## Live demo time!

- `r/google_compute_engine_demo.R`

- `r/ucsb_server_demo.R`

## Discussion

- What do folks think of our recommendation to generally use Google VMs with Compute Engine?
- Should we update the [emLab SOP](https://emlab-ucsb.github.io/SOP/5-high-performance-computing.html) and make this more formal?
- We might eventually think about a standard emLab VM Docker image that includes our commonly used packages
- And maybe even an emLab R package to help automate some of this?
- Depending on how many people and how intensively we are going to be using this, we may need to think about how to better monitor and manage billing across projects